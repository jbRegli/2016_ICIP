% Template for ICIP-2013 paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%
% My usual settings %
%%%%%%%%%%%%%%%%%%%%%
\usepackage{JB_config_article}
\usepackage{spconf}

%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Location of the figures %
%%%%%%%%%%%%%%%%%%%%%%%%%%%
\graphicspath{{Images/}} 

% % Example definitions.
% % --------------------
% \def\x{{\mathbf x}}
% \def\L{{\cal L}}

% Title.
% ------
\title{SCATTERING CONVOLUTIONAL HIDDEN MARKOV TREE}
%
% Single address.
% ---------------
\name{J.B. REGLI, J. D. B. NELSON 
	\thanks{J.-B. Regli is funded by a Dstl/UCL Impact studentship. J. D. B. Nelson is partially supported by grants from the Dstl and Innovate UK/EPSRC.}
}
\address{UCL, Department of Statistical Science}
%
% For example:
% ------------
%\address{School\\
%  Department\\
%  Address}
%
\begin{document}
%\ninept
%
\maketitle
%
\begin{abstract}
  A Scattering convolutional hidden Markov tree proposes a new inference mechanism for high-dimensional signals by combining the interesting signal representation created by the scattering transform to a powerful probabilistic graphical model.
  A wavelet scattering network computes a translation invariant and stable to deformations representation of a signal while still preserving its informative content. Such properties are acquired by cascading wavelet transform convolutions with non-linear modulus and averaging operators.
  The network's structure and its distributions are described using a Hidden Markov Tree. This yield a generative model for high-dimensional inference. It offers a mean for performing several inference tasks among which are predictions. The scattering convolutional hidden Markov tree displays promising results on classification tasks of complex images in the difficult case were the number of training examples is extremely limited.
\end{abstract}
%
\begin{keywords}
  Scattering network, Hidden Markov Model, Classification, Deep network
\end{keywords}
%
\section{Introduction}
\label{sec:Intro}

  The standard approach to classify high dimensional signals can be expressed as a two steps procedure. First the data are projected in a feature space where the task at hand is simplified. Then prediction is done using a simple predictor in this new representational space. The mapping can either be hand-built ---\eg Fourier transform, wavelet transform--- or learned. In the last decade methods for learning the projection have drastically improved under the impulsion of the so called deep learning. Deep neural networks (sometime enriched by convolutional architecture) have been able to learn very effective representations for a given dataset and a given task~\cite{salakhutdinov2009deep, vincent2010stacked, lecun1995convolutional}. Such method have achieved state of the art on many standard problems~\cite{krizhevsky2012imagenet, hinton2012improving} as well as real world applications~\cite{huval2015empirical}. 

  However deep learning methods are only efficient when we have access to a vast quantity of training examples~\cite{glorot2010understanding}. But in some cases, such as in medical or defence applications for example, datapoints are rare or using an expert for hand-labelling them is either time-consuming, costly or subjective. Hence in situations where training examples are expensive to collect, learning has to be performed on smaller datasets. In that case using a fix hand crafted set of filters seems to be one of the best solution~\cite{hsiang2001embedded}. Recently Mallat described the scattering transform~\cite{mallat2012gis} a fixed bank of wavelet filters used to generate data representation in a convolutional neural networks like architecture. This representational method associated to a support vector machine classifier achieved close to state of the art performances of some standard datasets~\cite{bruna2010classification}. Furthermore this method can be accurately applied to relatively smaller datasets~\cite{sifre2013rotation}.

  When an extremely low number of training examples are available ---one-shot learning~\cite{fei2006one}--- generative classification methods provide better performances than their discriminative counterparts~\cite{jordan2002discriminative}. Model a signal representation using generative probabilistic graphical models have been successfully done for various wavelet transform~\cite{crouse1998wavelet, kingsbury2001complex}. In those work Hidden Markov trees are used to model the wavelet coefficients distribution. 
  
  We propose a method combining the recently proposed deterministic analytically tractable transformation inspired by deep convolutional to a probabilistic graphical model in order to create a powerful probabilistic tool to handle high dimensional prediction problems. In a similar fashion to the work done by Crouse on wavelet trees~\cite{crouse1998wavelet}, we propose to describe Mallat's scattering convolutional network~\cite{bruna2010classification} using a hidden Markov tree. Replacing the wavelet by a scattering transform allows us to leverage its interesting representational properties but also force us to adapt the HMT model to non-homogeneous non regular trees. We develop a new framework to model high-dimensional inputs which, in contrast to passing the raw scattering coefficients into a classifier, captures dependencies between different layers in a generative probabilistic model and, unlike Crouse's HMT, our proposed framework accommodates invariances to deformations. Moreover as opposed to the commonly used simple classification method, once trained our model can tackle prediction problems but also other inference tasks ---\eg generation, sensitivity analysis... --- and can also outperform SVMs when only a very low number of training examples are available.

  The remainder of this paper introduces the scattering convolutional hidden Markov tree and is organised as follow. In section~\ref{sec:SCN} we review the scattering transform and some of its properties. Section~\ref{sec:SCHMT} introduces the proposed Scattering Hidden Markov Tree (SCHMT). Section~\ref{sec:Exps} we perform classification on a selection of standard datasets restricted to only a few training samples. We draw conclusions in Section~\ref{sec:Conclusion}.
  
\section{Scattering networks}
  \label{sec:SCN}
    Scattering convolutional networks (SCNs)~\cite{bruna2013scattering} are Convolutional Neural Networks (CNNs)~\cite{lecun1995convolutional} using a fixed filter bank of wavelets. Those filters can be hand-crafted to yield descriptors with the desired invariances~\cite{mallat2012gis}. For image classification tasks, one is interested in descriptors that are ---at least--- stable to deformations and invariant to translations. Note that SCNs producing more complexes set of invariances exist but on the remainder of this paper we consider only on descriptors with the previously mentioned properties.

  \subsection{Scattering transform}
    \label{subsec:SCN/ST} 
    Wavelets are localized functions stable to deformations. They are thus well adapted to construct descriptor that would also be translation invariant. A two-dimensional spatial wavelet transform $W$ is obtained by scaling by $2^{j}$ and rotating by $r_{\theta}$ a mother wavelet $\psi$,
    \vspace{-5pt}
    \begin{equation}
      \label{eq:multi-scale directional wavelet}
      \psi_{\lambda}(u) = \psi_{j,\theta}(u) = 2^{-2j} \psi(2^{-j}r_{\theta}p)
    \end{equation}
    
    In the remainder of this paper we restrict to Morlet wavelet transforms defined on $\Lambda = G \times \llbracket 0,J \rrbracket$ where $G$ is a finite group of rotations of cardinal $L$ and where the wavelet is taken at scale $J$,
    \vspace{-5pt}
    \begin{equation}
      \label{eq:wavelet transform}
      W_{J}\bfx = \{ \bfx \ast \phi_{J}(u); \bfx \ast \psi_{\lambda}(u) \}_{p\in\dsR^{2},\lambda \in \Lambda}
    \end{equation}
    \vspace{-15pt}

    While the averaging part $\phi_{J}$ of the wavelet transform is invariant to translations, the high frequency part $\psi_{\lambda}$ is covariant to them~\cite{mallat2012gis}. Invariance within a limited range inferior $2^{J}$ can be achieved by averaging the positive envelope with a smooth window,
    \vspace{-5pt}
    \begin{equation}
      \label{eq:Scattering transform}
      S_{J}[\lambda]\bfx(u) = \abs{\bfx \ast \psi_{\lambda}} \ast \phi_{J}(u)
    \end{equation}
    \vspace{-10pt}
    
    Such non-linearised averaged wavelet coefficients are used in various form in computer vision (SIFT~\cite{grabner2006fast}, DAISY~\cite{winder2009picking}), but the scattering transform proposes a new non-linearity as well as a layer based architecture.
  
  \subsection{Scattering convolutional network}
    \label{subsec:SCN/SCN}
    While providing local translation invariance, the averaging convolution introduced in~\ref{eq:Scattering transform} also removes the spatial variability of the wavelet transform. SCNs cascade this wavelet modulus operator to recover the lost information and compute progressively more invariant descriptors. Let combine the wavelet transform and modulus operations into a single wavelet modulus operator,
    \vspace{-5pt}
    \begin{equation}
      \mcalU_{J} \bfx = \{ S[\emptyset] \bfx ; U[\lambda]\bfx \}_{\lambda \in \Lambda_{J}} 
          = \{ \bfx \ast \phi_{J}; \abs{\bfx \ast \psi_{\lambda}} \}_{\lambda \in \Lambda_{J}},
      \label{eq:One step propagator}
    \end{equation}
    
    A scattering transform can be interpreted as a CNN~\cite{oyallon2014deep} illustrated in Figure~\ref{fig:SCN 1} which propagates a signal $\bfx$ across multiple layers of the network and which outputs at each layer $m$ the scattering invariant coefficients $S[p_{m}]\bfx$ where $p_{m}=(\lambda_{1} \dots \lambda_{m})$ is a path of $m$ orientations and scales.
    
    \begin{figure}
      \begin{center}
        \includegraphics[width=3.3in, height=2in, keepaspectratio]{scatnet_crop.pdf}
        \caption[Scattering convolution network.]{\centering  Scattering networks can be seen as neural networks iterating over wavelet  modulus  operators $\mcalU_{j}$. Each layer $m$ outputs the averaged  invariants $S[p_{m}]\bfx$ and covariant coefficients $U[p_{m+1}]\bfx$.}
        \label{fig:SCN 1}
        % TODO: Change it for something similar to Siffre rigid motion
      \end{center}
      \vspace{-15pt}
    \end{figure}
    
    The scattering energy is mainly concentrated along frequency decreasing paths, \ie for which $\abs{\lambda_{k+1}} \leq \abs{\lambda_{k}}$~\cite{mallat2012gis}. The energy contained in the other paths is negligible and thus for applications only frequency decreasing paths are considered. Moreover there exist a path length $M > 0$ after which all longer paths can be neglected. For signal processing applications, this decay appears to be exponential. And for classification applications, paths of length $M = 3$, \ie two convolutions, provides the most interesting results~\cite{anden2011multiscale, bruna2010classification}.
      
    This restrictions yield an easier parametrization of a scattering network. Indeed its now completely defined by the mother wavelet $\phi$, the maximum path length considered $M$, the finest scale level considered $J$ and the number of orientation considered $L$.
      
    Hence for a given set of parameter $(\psi, M,J,L)$, let $ST_{(\psi, M,J,L)}(\bfx)$ denotes the unique frequency decreasing windowed scattering convolutional network with those parameters evaluated for signal $\bfx$. Each node $i$ of this network generates a -possibly empty- set of of nodes of size $(j_{i}-1) \times L$ where $j_{i}$ is the scale of node $i$ and $L$ is the number of orientations considered and it has the architecture displayed by Figure~\ref{fig:SCN 2}.

    \begin{figure}
      \begin{center}
        \includegraphics[width=3.3in, height=2in, keepaspectratio]{ST_freqDec_crop.pdf}
        \caption[Frequency decreasing scattering convolution network.]{\centering  Frequency decreasing scattering convolution network with $J=4$, $L=1$ and $M=2$. A node $i$ at scale $j_{i}$ generates $(j_{i}-1) \times L$ nodes. }
        \label{fig:SCN 2}
        % TODO: Narrow the gaps
      \end{center}
      \vspace{-15pt}
    \end{figure}
    
  \subsection{Scattering convolutional classifier}
    \label{subsec:SCN/SCC}
    In the original framework~\cite{bruna2013scattering}, the scattering network $ST_{(\psi, M,J,L)}(.)$ is used for classification task using a SVM classifier on the outputs of the network. Performance can be slightly improved by adding a feature selection step performing PCA on the scattering coefficients and keeping only the most informative ones. This classification framework provides results comparable with the state of the art on several datasets~\cite{bruna2010classification}.

\section{The Scattering hidden Markov tree}
  \label{sec:SCHMT}
  State of the art performance can be achieved using SCNs associated to SVMs. However this approach is not adapted to very small training sets or to deliver a probabilistic output. To overcome those limitations we propose an adaptation of Crouse~\cite{crouse1998wavelet} and Durand~\cite{durand2004computational} wavelet hidden Markov trees to the non-regular non-homogeneous tree structure of SCNs.
  %SCNs associated to a SVM classifier achieve good performance on classification task. However SVMs are not adapted to extremely limited number of training samples. They also provide only boolean labeling. The output of an SVM can be expressed as a probability~\cite{platt1999probabilistic} but it is a rescaling more than true probability. If one is interested in a true probabilistic models of the scattering coefficients, it is quite natural to express them as a probabilistic graphical model. We thus propose an adaptation of the wavelet hidden Markov trees~\cite{Crouse and Durand} to non-regular non-homogeneous tree structure of SCNs.
  
  \subsection{Hidden Markov tree model}
    \label{subsec:SCHMT/HMT model}
    The HMT models the marginal distribution of each real ST coefficient $S_i$ as a Gaussian mixture. To each $S_i$, we associate a discrete hidden state $H_i$ that takes on values in $\llbracket 1,K \rrbracket$ with probability mass function (pmf) $P(H_i)$. Conditioned on $H_i = k$, $S_i$ is Gaussian with mean $\mu_{i,k}$ and variance $\sigma_{i,k}$. Thus, its overall marginal PDF is given by $P(w_i) = \sum_{k=1}^{K} P(H_i = m)P(S_i| H_i=k)$ with $P(S_i|H_i=k) \sim \mcalN(\mu_{i,k}, \sigma_{i,k})$. While each scattering coefficient $S_i$ is conditionally gaussian given its state variable $H_i$, overall it has a non-Gaussian density. Finally the probability for the hidden node $H_{i}$ to be in a state $k$ given its father's state $g$ is characterized by a transition probability such that $\epsilon_{i}^{(gk)} = P(H_{i}= k | H_{\rho(i)}=g)$. This yields $P(H_{i}=k) = \sum_{g=1}^{K} \epsilon_{i}^{(gk)} P(H_{\rho(i)}=g)$.
    
%     The initial state is drawn from a discrete non uniform distribution $\pi_{0}$ such that $\pi_{0}(k) = P(H_{0}=k)$.
%     For any index $i$ of the tree, the emission distribution describes the probability of the visible node $S_{i}$ conditional to the hidden state $H_{i}$ and $P(S_{i}=s_{i}|H_{i}=k) = P_{\theta_{k,i}}(s)$ where $P_{\theta_{k,i}}$ belongs to a parametric distribution family and $\theta_{k,i}$ is the vector of emission parameters for the state $k$ and node $i$. In the remainder of the paper the emission distribution is Gaussian so that $P(S_{i}=s | H_{i}=k) = \mcalN(\mu_{k,i},\sigma_{k,i})$, where $\theta_{k,i}=(\mu_{k,i},\sigma_{k,i})$ with $\mu_{k,i}$ and $\sigma_{k,i}$ being respectively the mean and the variance of the Gaussian for the $k$-th value of the mixture and the node $i$.
%     Finally the probability for the hidden node $H_{i}$ to be in a state $k$ given its father's state $g$ is characterized by a transition probability such that $\epsilon_{i}^{(gk)} = P(H_{i}= k | H_{\rho(i)}=g)$ where $\epsilon_{i}$ defines a transition probability matrix such that $P(H_{i}=k) = \sum_{g=1}^{K} \epsilon_{i}^{(gk)} P(H_{\rho(i)}=g)$.

    Such a model is pictured in Figure~\ref{fig:SCHMT 1} and for a given scattering architecture ---\ie fixed $M$, $J$ and $L$--- the SCHMT model is fully parametrized by,
    \vspace{-5pt}
    \begin{equation}
      \Theta = \big(\pi_{0}, \{ \epsilon_{i}, \{ \theta_{k,i} \}_{k\in\llbracket1,K\rrbracket} \}_{i\in\mcalT}\big).
      \label{eq:SCHMT - parameters}
    \end{equation}
    \vspace{-15pt}

    \begin{figure}
      \begin{center}
        \includegraphics[width=3.3in, height=2in, keepaspectratio]{scat_HMT_crop.pdf}
        \caption{Scattering convolutional hidden Markov tree.}
        \label{fig:SCHMT 1}
        % TODO: Narrow the gaps
      \end{center}
      \vspace{-15pt}
    \end{figure}
    
    This model implies two assumptions on the scattering transform. First --- $K$-populations--- that a signal’s scattering coefficients can be described by $K$ clusters. This is a common assumptions for standard wavelets~\cite{kingsbury2001complex} and hence it can be extended to the scattering transform. The SCHMT also assumed ---persistence--- that the informative character of a coefficients is propagated across layers. This assumption is sound since scattering coefficients are highly correlated~\cite{oyallon2014deep}.
    
  \subsection{Learning the tree parameters}
    \label{subsec:SCHMT/Learning}    

    SCHMT is trained using the smoothed version of the Expectation-Maximization (EM) algorithm for hidden Markov trees proposed by~\cite{durand2001statistical} and adapted to non-homogeneous and non-binary trees.
    
    Let $\bar{\mcalS}_{i}= \bar{s}_{i}$ be the observed sub-tree rooted at node $i$. By convention $\bar{\mcalS}_{0}$ denotes the entire observed tree. The smoothed version of the E-step requires the computation of the conditional probability distributions $\xi_{i}(k) = P(H_{i}=k | \bar{\mcalS}_{i}= \bar{s}_{i})$ (smoothed probability) and $P(H_{i}=k, H_{\rho(i)}=g | \bar{\mcalS}_{i}= \bar{s}_{i})$ for each node $i \in \mcalT$ and states $k$ and $g$. This can be achieved through an upward-downward recursion displayed in Algorithm~\ref{algo:Smoothed upward} and~\ref{algo:Smoothed downward}. The output from the downward step are used M-step as showed in Algorithm~\ref{algo:Mstep}.
    
    \vspace{-15pt}
    \setlength{\algomargin}{+4pt}
    \begin{center}
      \begin{algorithm}
				\vspace{-3pt}
				\fontsize{8pt}{10pt}\selectfont
%          \textbf{Meta-parameters:}\\
%         $K$\\
        \tcp{Initialization:}
%         \tcp{$P_{\theta_{k,i}}(s_{i})$:}
        \For{All the nodes $i$ of the tree $\mcalT$}{ 
          $P_{\theta_{k,i}}(s_{i}) = \mcalN(s_{i} | \mu_{k,i},\sigma_{k,i})$
        }
%         \tcp{Loop over the leaves $i$ of the tree:}
        \For{All the leaves $i$ of the tree $\mcalT$}{
          $\beta_{i}(k) = \frac{P_{\theta_{k,i}}(s_{i}) P(H_{i}=k)}{\sum_{g=1}^{K} P_{\theta_{g,i}}(s_{i}) P(H_{i}=g)}$\\
          $\beta_{i,\rho(i)}(k) = \sum_{g=1}^{K}\frac{\beta_{i}(g) \epsilon_{i}^{(kg)}}{P(H_{i}=g)} . P(H_{\rho(i)}=k)$ \\
          $l_{i} = 0$
        }
        \tcp{Induction:}
%         \tcp{Bottom-Up loop over the nodes of the tree:}
        \For{All non-leaf nodes $i$ of the tree $\mcalT$ (Bottom-up)}{
          $M_{i} = \sum_{k=1}^{K} P_{\theta_{k,i}}(s_{i}) \prod_{j \in c(i)} \frac{\beta_{j,i}(k)}{P(H_{i}=k)^{n_{i}-1}}$ \\
          $l_{i} = \log(M_{i}) + \sum_{j \in c(i)}l_{j}$\\
          $\beta_{i}(k) = \frac{P_{\theta_{k,i}(s_{i})} \prod_{j \in c(i)}(\beta_{j,i}(k))}{P(H_{i}=k) ^{n_{i}-1} M_{i}} $\\
          \For{All the children nodes $j$ of node $i$}{
            $\beta_{i\backslash c(i)}(k) = \frac{\beta_{i}(k)}{\beta_{i,j}(k)}$
          }
          $\beta_{i,\rho(i)}(k) = \sum_{g=1}^{K} \frac{\beta_{i}(g) \epsilon_{i}^{(kg)}}{P(H_{i}=g)} .P(H_{\rho(i)}=k)$
        }
        \caption{Smoothed upward algorithm.}
        \label{algo:Smoothed upward}
        \vspace{-5pt}
      \end{algorithm}        
    \end{center}
    \vspace{-30pt}
    \vspace{-20pt}  
    \begin{center}
      \begin{algorithm}
				\vspace{-3pt}
				\fontsize{8pt}{10pt}\selectfont
%         \textbf{Meta-parameters:}\\
%         $K$\\
        \tcp{Initialization:}
        $\alpha_{0}(k) = 1$\\
        \tcp{Induction:}
%         \tcp{Top-Down loop over the nodes of the tree:}
        \For{All nodes $i$ of the tree $\mcalT\backslash\{0\}$ (Top-Down)}{
          $\alpha_{i}(k) = \frac{1}{P(H_{i}=k)} \sum_{g=1}^{K} \alpha_{\rho(i)}(g) \epsilon_{i}^{(gk)} \beta_{\rho(i)\backslash i}(g) P(H_{\rho(i)}=g)$
        }
        \caption{Smoothed downward algorithm.}
        \label{algo:Smoothed downward}  
				\vspace{-5pt}
      \end{algorithm}        
    \end{center}
    \vspace{-30pt}
    \vspace{-20pt}  
    \begin{center}
      \begin{algorithm}
			\vspace{-3pt}
			\fontsize{8pt}{10pt}\selectfont
%         \textbf{Meta-parameters:}\\
%         $K$,\\
%         Distribution family for $P_{\theta}$ \tcp*{Here Gaussian}
%         $N$ \tcp*{Number of observed realizations of the signal}
        \tcp{Initialization:}
        $\pi_{0}(k) = \frac{1}{N} \sum_{n=1}^{N} P(H_{0}^{n}=m|s_{0}^{n},\Theta^{l})$\\
        \tcp{Induction:}
%         \tcp{Loop over the nodes of the tree:}
        \For{All nodes $i$ of the tree $\mcalT\backslash\{0\}$}{
          $P(H_{i}=k) = \frac{1}{N} \sum_{n=1}^{N} P(H_{i}^{n}=k|\bar{s}_{0}^{n},\Theta^{l})$,\\
          $\epsilon_{i}^{gk} = \frac{\sum_{n=1}^{N} P(H_{i}^{n} = k, H_{\rho(i)}^{n}=g |\bar{s}_{0}^{n}, \Theta^{l})} {N P(H_{\rho(i)}=k)}$,\\
          $\mu_{k,i} = \frac{\sum_{n=1}^{N} s_{i}^{n} P(H_{i}^{n} = k |\bar{s}_{0}^{n}, \Theta^{l})} {N P(H_{i}=k)}$,\\
          $\sigma_{k,i}^{2} = \frac{\sum_{n=1}^{N} (s_{i}^{n} - \mu_{k,i})^{2} P(H_{i}^{n} = k |\bar{s}_{0}^{n}, \Theta^{l})} {N P(H_{i}=k)}$.
        }
        \caption{M-step of the EM algorithm.}
        \label{algo:Mstep}
				\vspace{-5pt}
      \end{algorithm}        
    \end{center}
    \vspace{-40pt}
    
	\subsection{MAP classification}
    \label{subsec:SCN/MAP}
    
     Let $\Theta_{c}$ now be a set of parameters for an SCHMT $\mcalT$ learned on a training set $\{\bar{S}_{0,c}^n\}_{n \in \llbracket1, N \rrbracket} = \{ST_{(\psi, J, M, L)}(\bfx_{c}^{n})\}_{n \in \llbracket1, N \rrbracket}$ composed of the scattering representations of $N$ realizations of a signal of class $c$ . Let also $\bfx^{new}$ be another realization of this signal, not used for training and $\mcalT^{new}$ be the instance of the SCHMT generated by this realization.
     
     In this context the MAP algorithm~\cite{durand2001statistical} aims at finding the optimal hidden tree $\hat{\bar{h}}_{0}^{new}=(\hat{h}_{0}^{new} \dots \hat{h}_{I-1}^{new})$ maximizing the probability of this sequence given the model's parameters $P(\bar{\mcalH}_{0}= \hat{\bar{h}}_{0}^{new}|\mcalT^{new},\Theta_{c})$. The MAP framework also provides $\hat{P}$ the value of this maximum.
    
    The MAP algorithm can be used in a multi-class classification problem by training an SCHMT model per class and then when presented with a new realization $\bfx^{new}$ comparing the probability of the MAP hidden tree provided by each model
    
\section{Classification results}
  \label{sec:Exps}
  
  We compare the performance of SCHMT to those of a SCN combined to an SVM (SCN+SVM) on restrictions to small number of training examples of two standard datasets. For both datasets and classification methods we use a scattering transform with $M=3$ orders, $J=5$ scales, $L=3$ orientations and a Morlet mother wavelet. The hidden Markov tree has $K=2$ states and is using a mixture of Gaussian to describe the relationship between the scattering coefficients and the hidden states. For the SVM, the parameters are selected by cross-validation.
  
  \subsection{MNIST}
		\label{subsec:Exps/MNIST}
		We first test SCHMTs on the digit classification dataset MNIST~\cite{lecun2016web}. SCHMT and SCN+SVM are both trained on a limited number of training examples per class of MNIST and tested on $200$ test samples ($20$ per class). The two methods use  This experiment is run $100$ times per number of training samples and results are displayed in Figure~\ref{fig:SCHMT MNIST}.

		\begin{figure}
% 			\vspace{-20pt}
			\begin{center}
				\includegraphics[width=3in, height= 1in]{placeholder.jpg}
			  \caption[MNIST: classification scores]{Average and best classification scores on MNIST trained on $N=2,5,10,20$ training examples per class.}
			  % To curve with schmt and scn+svm clf score (avg + max) 
			  %TODO: real figure
			  \label{fig:SCHMT MNIST}
			\end{center}
			\vspace{-20pt}
		\end{figure}
		When the number of training example is really limited ---\ie $2$ or $5$--- are slightly higher for SCHMT compare to SCN+SVM. However the EM algorithm performances are undermined by convergence to local minima issues~\cite{moon1996expectation}. When convergence occurs correctly SCHMTs reach much better performances than the best SVMs --- $80\%$ compared to $54\%$ for $5$ training examples. Finally, as expected, when the number of training samples grow large enough ---\ie $10$ and onward--- for the SVMs, SCN+SVMs reach both better maximum and average score.
		
		To assess the generalization quality even further, we test the best SCHMT and SCN+SVM models trained on $5$ images per class on the full $10000$ test samples. %TODO: results + ccl
		
		This experiment confirm the superiority of generative model for limited number of training points. It also highly some weakness of the SCHMTs as sometime convergence is problematic and the problem seems to get stronger when the number of training samples increases. However when convergence occurs correctly SCHMT provides great classification score for low number of training examples.
		
	\subsection{KTH Texture}
		\label{subsec:Exps/KTH Texture}
		
		Further experiments are run on the KTH-TIPS texture dataset~\cite{KTH2016web}. We perform the classification using $5$ training examples per class and perform the testing on the rest of the dataset, \ie $155$ samples per class. SVM's parameters are optimized using cross-validation.
		
		Donec ut luctus est. Mauris pellentesque, lectus ornare luctus tincidunt, lorem leo ultrices nunc, efficitur suscipit nunc mauris at ipsum. Donec eros nibh, rhoncus at mi id, mattis varius magna. Nullam luctus nisl nisl, sit amet tincidunt turpis venenatis et. Nam sem lectus, feugiat eget nisl a, dapibus vulputate urna. Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Morbi tincidunt aliquet dolor, eu efficitur tortor tempus id. Quisque vestibulum sapien non metus finibus, ut eleifend leo molestie.

\section{Conclusion}
  \label{sec:Conclusion}
  SCHMTs propose a new framework to process a high-dimensional signal from the raw data to the prediction task. The scattering transform projects the data into a representational space of even higher dimensionality but of reduced volume along the invariants in the data. Then a probabilistic graphical model ---hidden Markov tree--- is used to fit a generative model to the distribution of the representation of the data. This yield a powerful tool combining the interesting properties of the scattering transform for signal representation and the representational power of the hidden Markov models. The modeled distribution can be used to perform efficient classification tasks even given low information. Even though this document considers only classification, a generative model is much more versatile than a simple ---yet efficient--- discriminative counterpart. Because they model the full distribution of the data they can express more complex relationships between the observed and the unknown variables than simple discrimination. 
  
  To enhance SCHMT and especially the chance of converging toward a good minima during the EM learning, we plan on developing a variational to learn the model parameters~\cite{wainwright2008graphical}.


% To start a new column (but not a new page) and help balance the last-page
% column length use \vfill\pagebreak.
% -------------------------------------------------------------------------
\vfill
\pagebreak

% \section{COPYRIGHT FORMS}
% \l1abel{sec:copyright}
% 
% You must include your fully completed, signed IEEE copyright release form when
% form when you submit your paper. We {\bf must} have this form before your paper
% can be published in the proceedings.

% \section{REFERENCES}
% \label{sec:ref}
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% -------------------------------------------------------------------------
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

% \bibliography{bib_icip}

\end{document}
